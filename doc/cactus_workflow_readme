README for running "cactus_workflow.py"

==== Overview: ====

"cactus_workflow.py" is a python script that drives the Reconstruction 
Pipeline. In conjunction with the jobTree management system, the task of 
computing alignments, building trees and calculating ancestral 
adjacencies for a set of related sequences is made computationally 
tractible at the mammalian-scale using a cluster farm.

==== Installation: ====

*) Read the file "doc/README.txt".
	This file lists the environmental variables which you will need to 
set, as well as listing the programs which will need to be installed and the 
required minimum versions of various key programs.

==== Running: ====

*) Brief intro to jobTree
	cactus_workflow.py is run via the jobTree batch management 
system. jobTree is a system which builds upon existing batch management 
systems. Currently jobTree utilizes parasol to run cluster jobs, but is 
readily extensible to other systems (like LSF, Condor, etc). Further 
documentation on jobTree can be found in the file 
"src/workflow/jobTree/doc/README". 

	"jobTree.py" is the main driver program for the jobTree system. 
You can see the full list of command-line arguments used by "jobTree.py" 
by typing "jobTree.py --help". The relevant command-line parameters 
needed to run "cactus_workflow.py" are:

	--logDebug      	Turn on logging to output debug info
	--logFile=LOGFILE	Save logging info to the file LOGFILE
	--jobTree=JOBTREE	Save config files to the dir JOBTREE
	--batchSystem=BATCHSYSTEM	Specify the batch system, 
				currently either 'single_machine' or 
				'parasol'
	--command=COMMAND	Execute the command COMMAND on the cluster

Putting this all together, we run "jobTree.py" as follows:

$ jobTree.py --logDebug --logFile LOGFILE --jobTree JOBTREE \ 
    --batchSystem parasol --command COMMAND

	An important partner progam to "jobTree.py" is 
"jobTreeStatus.py" which you can use to monitor the state of the jobs 
run by "jobTree.py". The list of command-line arguments is shown by 
"--help". The typical use of "jobTreeStatus.py" is as follows:

$ jobTreeStatus.py --jobTree JOBTREE --verbose

This command will list the verbosely list the status of the jobs that 
are attached to the JOBTREE dir.

	Now that we have covered jobTree, we will next move onto a 
description of "cactus_workflow.py" which makes up the COMMAND value 
that we pass in.

*) "cactus_workflow.py" drives the pipeline and handles the task of 
building alignments, trees and adjacencies. As of September 2010, the accepted 
command-line arguments for cactus_workflow.py are shown below:

Usage: cactus_workflow.py [options] [sequence files]

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  --logInfo             Turn on logging at INFO level
  --logDebug            Turn on logging at DEBUG level
  --logLevel=LOGLEVEL   Log at level (may be either INFO/DEBUG/CRITICAL)
  --tempDirRoot=TEMPDIRROOT
                        Path to where temporary directory containing all temp
                        files are created, by default uses the current working
                        directory as the base
  --logFile=LOGFILE     File to log in
  --noRotatingLogging   Turn off rotating logging, which prevents log files
                        getting too big
  --job=JOBFILE         Job file containing command to run
  --experiment=EXPERIMENTFILE
                        The file containing a link to the experiment
                        parameters
  --setupAndBuildAlignments
                        Setup and build alignments then normalise the
                        resulting structure
  --buildTrees          Build trees
  --buildFaces          Build adjacencies
  --buildReference      Creates a reference ordering for the flowers

	There are four key phases in the reconstruction pipeline and 
can be broken down into the alignment phase, tree phase, reference phase and AVG 
phase. They directly map to the 4 following command-line arguments for 
"cactus_workflow.py":

	--setupAndBuildAlignments
	--buildTrees
	--buildReference
	--buildFaces

The phases have dependencies. The tree, reference and AVG phase all rely on 
having run the initial alignment phase. The reference and faces phase also rely on tree phase.

Only the phases specified to "cactus_workflow.py" will 
be run and each time a phase is run the output (see below) is updated. As a 
concrete example if you wanted to run all four phases you would specify:

	--setupAndBuildAlignments --buildTrees --buildReference --buildFaces

If you only wanted alignments and trees you'd leave off the final 
two command-line arguments like so:

	--setupAndBuildAlignments --buildTrees
	
The output of the pipeline is stored in a database, using either Tokyo Cabinet
or Mysql. The database is specified in a 'experiment' file, an XML format file
passed to the workflow which records the parameters of the experiment.

	In addition to the specifics of the database, there are several pieces of 
	information that need to be 
specified for any given analysis. A reconstruction pipeline experiment
requires a group of related sequences in FASTA format and a Newick tree 
representing the relationship between the sequences. 
	For the sake of illustration, let's imagine that we want to 
analyze the following sequences:

	DOG.fasta
	HUMAN.fasta
	MOUSE.fasta

related by the tree:

	"(DOG:0.197381, (HUMAN:0.14226, MOUSE:0.33316):0.02326);"
	
and putting the results in a Tokyo-cabinet database whose location is "foo/cactusDisk"
	
We would create the file "experiment.xml":

<cactus_workflow_experiment config="default" sequences="DOG.fasta HUMAN.fasta MOUSE.fasta" species_tree="(A:0.1,B:0.2):0.1,C:0.5);">
<cactus_disk>
<st_kv_database_conf type="tokyo_cabinet"><tokyo_cabinet database_dir="foo/cactusDisk" /></st_kv_database_conf>
</cactus_disk>
</cactus_workflow_experiment>

The "config" attribute specifies what parameters we should use in running the pipeline. 
By specifying "default" we will use the default parameters, which are stored in
src/cactus/pipeline/cactus_workflow_config.xml". A discussion of these parameters 
is beyond the scope of this readme!

The "sequences" attribute gives the sequences, which must be ordered left-to-right as
they are in the newick-tree string, contained in the "species_tree" attribute. In this 
example, the ordering of the sequences must be DOG, then HUMAN followed 
by MOUSE since that's how they read left to right in the tree.

The <cactus_disk> tag contains the details of the database, in this case the important
attributes are "type" which must equal either "tokyo_cabinet" or "mysql", and the 
"database_dir" attribute, which gives the location of the tokyo cabinet database.

If the experiment has used mysql the st_kv_database_conf tags would have had different
attributes. For example:

<st_kv_database_conf type="mysql"><mysql host="kolossus-10" port="0" user="cactus" 
password="cactus" database_name="cactus"/></st_kv_database_conf>

Would use the database server hosted on kolossus-10, with port=0 being the default, 
and the user password and database name all being "cactus".

Having described the experiment file we are in a position to describe a complete command
to run the pipeline. We utilize "jobTree.py" as described in the 
previous section, but now specify the format of COMMAND that we pass in 
via the "--command" argument. For this example we want to run the full 
pipeline, so the value of COMMAND is:

$ cactus_workflow.py --job JOB_FILE \ 
    --experiment ./experiment.xml --setupAndBuildAlignments --buildTrees --buildAdjacencies

It's worth noting that the JOB_FILE following --job is the literal 
string needed by the program - this is contrary to all the other times 
we have used upper-case to denote a variable. 

Now we can combine the "cactus_workflow.py" command with "jobTree.py" to 
get this:

$ jobTree.py --logDebug --logFile output.log --jobTree jobtree \ 
    --batchSystem parasol \ 
    --command "cactus_workflow.py --job JOB_FILE \ 
    --experiment ./experiment.xml --setupAndBuildAlignments --buildTrees --buildAdjacencies"

The above command is intended to be run on the main node of a parasol 
cluster farm and will initiate the jobTree jobs necessary to run the 
reconstruction pipeline on the HUMAN, MOUSE and DOG sequences.

==== CONCLUSION AND CONTACTS: ====

This readme file was written to be a quick and concise overview for how 
to run "cactus_workflow.py" using the jobTree cluster system. Please 
feel free to contact the authors for any followup questions.

Benedict Paten (benedict@soe.ucsc.edu)
Daniel Zerbino (dzerbino@soe.ucsc.edu)
Bernard Suh (bsuh@soe.ucsc.edu)
