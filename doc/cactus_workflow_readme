README for running "cactus_workflow.py"

==== Overview: ====

"cactus_workflow.py" is a python script that drives the Reconstruction 
Pipeline. In conjunction with the jobTree management system, the task of 
computing alignments, building trees and calculating ancestral 
adjacencies for a set of related sequences is made computationally 
tractible at the mammalian-scale using a cluster farm.

==== Installation: ====

*) Read the file "doc/README.txt".
	This file lists the environmental variables which you will need to 
set, as well as listing the programs which will need to be installed and the 
required minimum versions of various key programs.

==== Running: ====

*) Brief intro to jobTree
	cactus_workflow.py is run via the jobTree batch management 
system. jobTree is a system which builds upon existing batch management 
systems. Currently jobTree utilizes parasol to run cluster jobs, but is 
readily extensible to other systems (like LSF, Condor, etc). Further 
documentation on jobTree can be found in the file 
"src/workflow/jobTree/doc/README". 

	"jobTree.py" is the main driver program for the jobTree system. 
You can see the full list of command-line arguments used by "jobTree.py" 
by typing "jobTree.py --help". The relevant command-line parameters 
needed to run "cactus_workflow.py" are:

	--logDebug      	Turn on logging to output debug info
	--logFile=LOGFILE	Save logging info to the file LOGFILE
	--jobTree=JOBTREE	Save config files to the dir JOBTREE
	--batchSystem=BATCHSYSTEM	Specify the batch system, 
				currently either 'single_machine' or 
				'parasol'
	--command=COMMAND	Execute the command COMMAND on the cluster

Putting this all together, we run "jobTree.py" as follows:

$ jobTree.py --logDebug --logFile LOGFILE --jobTree JOBTREE \ 
    --batchSystem parasol --command COMMAND

	An important partner progam to "jobTree.py" is 
"jobTreeStatus.py" which you can use to monitor the state of the jobs 
run by "jobTree.py". The list of command-line arguments is shown by 
"--help". The typical use of "jobTreeStatus.py" is as follows:

$ jobTreeStatus.py --jobTree JOBTREE --verbose

This command will list the verbosely list the status of the jobs that 
are attached to the JOBTREE dir.

	Now that we have covered jobTree, we will next move onto a 
description of "cactus_workflow.py" which makes up the COMMAND value 
that we pass in.

*) "cactus_workflow.py" drives the pipeline and handles the task of 
building alignments, trees and adjacencies. As March 2010, the accepted 
command-line arguments for cactus_workflow.py are shown below:

Usage: cactus_workflow.py [options]

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  --logInfo             Turn on logging at INFO level
  --logDebug            Turn on logging at DEBUG level
  --logLevel=LOGLEVEL   Log at level (may be either INFO/DEBUG/CRITICAL)
  --tempDirRoot=TEMPDIRROOT
                        Path to where temporary directory containing all temp
                        files are created, by default uses the current working
                        directory as the base
  --logFile=LOGFILE     File to log in
  --noRotatingLogging   Turn off rotating logging, which prevents log files
                        getting too big
  --job=JOBFILE         Job file containing command to run
  --speciesTree=SPECIESTREE
                        The species tree relating the input sequences
  --netDisk=NETDISK     The location of the net disk.
  --maxIteraton=MAXITERATION
                        The maximum iteration to align to (0-4)..
  --setupAndBuildAlignments
                        Setup and build alignments
  --buildTrees          Build trees
  --buildAdjacencies    Build adjacencies

	There are three key phases in the reconstruction pipeline and 
can be broken down into the alignment phase, tree phase and adjacency 
phase. They directly map to the 3 following command-line arguments for 
"cactus_workflow.py":

	--setupAndBuildAlignments
	--buildTrees
	--buildAdjacencies 

The three phases have a linear dependency in that the alignment phase 
must be run before the tree phase which must be run before the final 
adjacency phase. The results of each phase are stored in a file we refer 
to as a netdisk. Only the phases specified to "cactus_workflow.py" will 
be run and each time a phase is run the netdisk is updated. As a 
concrete example if you wanted to run all three phases you would specify:

	--setupAndBuildAlignments --buildTrees --buildAdjancencies

If you only wanted alignments and trees you'd leave off the final 
command-line argument like so:

	--setupAndBuildAlignments --buildTrees

	There are several pieces of information that need to be 
specified for any given analysis. A reconstruction pipeline analysis 
requires a group of related sequences in FASTA format and a Newick tree 
representing the relationship between the sequences. 
	For the sake of illustration, let's imagine that we want to 
analyze the following sequences:

	DOG.fasta
	HUMAN.fasta
	MOUSE.fasta

related by the tree:

	"(DOG:0.197381, (HUMAN:0.14226, MOUSE:0.33316):0.02326);"

The name of the netdisk file where we will be storing the results is 
named "results.netdisk". We utilize "jobTree.py" as described in the 
previous section, but now specify the format of COMMAND that we pass in 
via the "--command" argument. For this example we want to run the full 
pipeline, so the value of COMMAND is:

$ cactus_workflow.py --job JOB_FILE \ 
    --speciesTree "(DOG:0.197381, (HUMAN:0.14226, MOUSE:0.33316):0.02326);"
    --netDisk results.netdisk \ 
    DOG.fasta HUMAN.fasta MOUSE.fasta \
    --setupAndBuildAlignments --buildTrees --buildAdjacencies

It's worth noting that the JOB_FILE following --job is the literal 
string needed by the program - this is contrary to all the other times 
we have used upper-case to denote a variable. Also another important 
consideration is that the ordering of the sequences does matter. They 
must follow the left to right appearance in the species tree. In this 
example, the ordering of the sequences must be DOG, then HUMAN followed 
by MOUSE since that's how they read left to right in the tree.

Now we can combine the "cactus_workflow.py" command with "jobTree.py" to 
get this:

$ jobTree.py --logDebug --logFile output.log --jobTree jobtree \ 
    --batchSystem parasol \ 
    --command "cactus_workflow.py --job JOB_FILE \ 
      --speciesTree '(DOG:0.197381, (HUMAN:0.14226, MOUSE:0.33316):0.02326);'
      --netDisk results.netdisk \ 
      DOG.fasta HUMAN.fasta MOUSE.fasta \
      --setupAndBuildAlignments --buildTrees --buildAdjacencies"

The above command is intended to be run on the main node of a parasol 
cluster farm and will initiate the jobTree jobs necessary to run the 
reconstruction pipeline on the HUMAN, MOUSE and DOG sequences.

==== CONCLUSION AND CONTACTS: ====

This readme file was written to be a quick and concise overview for how 
to run "cactus_workflow.py" using the jobTree cluster system. Please 
feel free to contact the authors for any followup questions.

Benedict Paten (benedict@soe.ucsc.edu)
Daniel Zerbino (dzerbino@soe.ucsc.edu)
Bernard Suh (bsuh@soe.ucsc.edu)
